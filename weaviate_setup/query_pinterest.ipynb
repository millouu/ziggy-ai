{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jayesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Jayesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import CLIPProcessor, CLIPModel, SegformerImageProcessor, AutoModelForSemanticSegmentation , AutoFeatureExtractor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import weaviate\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "import base64\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"patrickjohncyh/fashion-clip\"\n",
    "model = CLIPModel.from_pretrained(checkpoint)\n",
    "processor = CLIPProcessor.from_pretrained(checkpoint)\n",
    "seg_processor = SegformerImageProcessor.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\n",
    "seg_model = AutoModelForSemanticSegmentation.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\n",
    "\n",
    "def getTextEmbeddings(text):\n",
    "\tinputs = processor(text=text , images=Image.new('RGB' , (72 , 72)), return_tensors=\"pt\", padding=True)\n",
    "\toutputs = model(**inputs , return_dict=True)\n",
    "\treturn outputs[\"text_embeds\"]\n",
    "\n",
    "embeddings = getTextEmbeddings(\"white formal outfit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = weaviate.Client(url=\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'gitHash': 'ddb8a43',\n",
       "  'name': 'node1',\n",
       "  'shards': [{'class': 'PinterestImages',\n",
       "    'name': 'T6aWnYa0mcx4',\n",
       "    'objectCount': 509},\n",
       "   {'class': 'PinterestTop', 'name': 'PJPTMwTnCrl9', 'objectCount': 509},\n",
       "   {'class': 'PinterestBottom', 'name': 'NIu2FDBr3MkB', 'objectCount': 509},\n",
       "   {'class': 'FlipkartProducts', 'name': 'oQcrqlaKza74', 'objectCount': 167}],\n",
       "  'stats': {'objectCount': 1694, 'shardCount': 4},\n",
       "  'status': 'HEALTHY',\n",
       "  'version': '1.19.8'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.cluster.get_nodes_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.schema.get(\"FlipkartProducts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = (\n",
    "    client.query\n",
    "    # .get(\"FlipkartProducts\",[\"uRL\", \"brand\", \"category\", \"product\", \"price\", \"rating\", \"numberRatings\", \"colour\", \"row\"])\n",
    "    .get(\"PinterestImages\", [\"image\", \"top{... on PinterestTop { image, _additional {vector} }}\"])\n",
    "    .with_near_vector({\"vector\" : embeddings.tolist()[0]})\n",
    "    # .with_near_vector({\"vector\" : top_embedding})\n",
    "    .with_additional([\"vector\", \"id\", \"distance\"])\n",
    "    .with_limit(7)\n",
    "    .do()\n",
    ")\n",
    "print(json.dumps(response, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipkart_embedding = response[\"data\"][\"Get\"][\"FlipkartProducts\"][0]['_additional'][\"vector\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_embedding = response[\"data\"][\"Get\"][\"PinterestImages\"][0][\"top\"][0]['_additional'][\"vector\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = (\n",
    "    client.query\n",
    "    .get(\"PinterestTop\",[\"image\"])\n",
    "    .with_near_vector({\"vector\" : top_embedding})\n",
    "    .with_additional([\"vector\", \"id\"])\n",
    "    .with_limit(3)\n",
    "    .do()\n",
    ")\n",
    "print(json.dumps(response, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = (\n",
    "    client.query\n",
    "    .get(\"FlipkartProducts\",[\"image\"])\n",
    "    .with_near_vector({\"vector\" : top_embedding})\n",
    "    .with_additional([\"vector\", \"id\", \"distance\"])\n",
    "    .with_limit(10)\n",
    "    .do()\n",
    ")\n",
    "print(json.dumps(response, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageEmbeddingsFromPath(image_path):\n",
    "\timage = Image.open(image_path)\n",
    "\tinputs = processor(text=[\"dummy\"] , images=image, return_tensors=\"pt\", padding=True)\n",
    "\toutputs = model(**inputs , return_dict=True)\t\n",
    "\treturn outputs[\"image_embeds\"]\n",
    "\n",
    "def getImageEmbeddings(image):\n",
    "\tinputs = processor(text=[\"dummy\"] , images=image, return_tensors=\"pt\", padding=True)\n",
    "\toutputs = model(**inputs , return_dict=True)\n",
    "\treturn outputs[\"image_embeds\"]\n",
    "\n",
    "def applyMask(image, mask):\n",
    "\timage = np.array(image)\n",
    "\tmask = np.array(mask)\n",
    "\tmask = np.stack((mask,)*3, axis=-1)\n",
    "\tresultant = image*mask\n",
    "\tresultant[mask == 0] = 255\n",
    "\treturn resultant\n",
    "\n",
    "def cropImage(image):\n",
    "\ttemp = image[:, :, ::-1].copy() \n",
    "\ttemp = temp.astype('uint8')\n",
    "\tgray = cv2.cvtColor(temp, cv2.COLOR_BGR2GRAY)\n",
    "\tthresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "\tcontours = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "\tcontours = sorted(contours, key=lambda x: cv2.contourArea(x), reverse=True)\n",
    "\tx,y,w,h = cv2.boundingRect(contours[0])\n",
    "\tcrop = image[y:y+h, x:x+w]\n",
    "\treturn crop\n",
    "\n",
    "def segment(image, to_mask):\n",
    "\tinputs = seg_processor(images=image, return_tensors=\"pt\")\n",
    "\toutputs = seg_model(**inputs)\n",
    "\tlogits = outputs.logits.cpu()\n",
    "\tupsampled_logits = nn.functional.interpolate(\n",
    "\t\tlogits,\n",
    "\t\tsize=image.size[::-1],\n",
    "\t\tmode=\"bilinear\",\n",
    "\t\talign_corners=False,\n",
    "\t)\n",
    "\tpred_seg = upsampled_logits.argmax(dim=1)[0]\n",
    "\tresult = []\n",
    "\tfor i in to_mask:\n",
    "\t\tmask = pred_seg.numpy().copy()\n",
    "\t\tmask[mask != i] = 0\n",
    "\t\tmask[mask == i] = 1\n",
    "\t\titem = applyMask(image, mask)\n",
    "\t\tresult.append(item)\n",
    "\treturn result\n",
    "\n",
    "def segmentAndEmbed(image_path, to_mask):\n",
    "\tresult = {}\n",
    "\timage = Image.open(image_path)\n",
    "\tfullImageEmbedding = getImageEmbeddings(image)\n",
    "\tbuffered = BytesIO()\n",
    "\timage.save(buffered, format=\"PNG\")\n",
    "\tfullImageBase64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "\tresult[\"fullImageBase64\"] = fullImageBase64\n",
    "\tresult[\"fullImageEmbedding\"] = fullImageEmbedding\n",
    "\tsegments = segment(image, to_mask)\n",
    "\tfor i in range(len(to_mask)):\n",
    "\t\tsegmentEmbedding = getImageEmbeddings(segments[i])\n",
    "\t\tsegments[i] = Image.fromarray(np.uint8(segments[i]))\n",
    "\t\tbuffered = BytesIO()\n",
    "\t\tsegments[i].save(buffered, format=\"PNG\")\n",
    "\t\tsegmentBase64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "\t\tresult[f\"segmentBase64_{i}\"] = segmentBase64\n",
    "\t\tresult[f\"segmentEmbedding_{i}\"] = segmentEmbedding\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = segmentAndEmbed(\"pinterest_image.jpg\", [4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_embedding1 = result[\"fullImageEmbedding\"].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = response[\"data\"][\"Get\"][\"FlipkartProducts\"][2]['image']\n",
    "# image = response[\"data\"][\"Get\"][\"PinterestTop\"][0]['image']\n",
    "# image = response[\"data\"][\"Get\"][\"PinterestImages\"][6]['image']\n",
    "image = Image.open(BytesIO(base64.b64decode(image.split(\",\",1)[0])))\n",
    "image.show()\n",
    "# image.save(\"pinterest_image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    # image = response[\"data\"][\"Get\"][\"FlipkartProducts\"][i]['image']\n",
    "    # image = response[\"data\"][\"Get\"][\"PinterestTop\"][0]['image']\n",
    "    # image = response[\"data\"][\"Get\"][\"PinterestImages\"][i]['image']\n",
    "    image = Image.open(BytesIO(base64.b64decode(image.split(\",\",1)[0])))\n",
    "    image.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
